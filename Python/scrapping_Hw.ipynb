{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "import locale\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.\n",
    "### Обязательная часть\n",
    "\n",
    "Будем парсить страницу со свежеми новостям на habr.com/ru/all/.\n",
    "\n",
    "Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. Эти слова определяем в начале кода в переменной, например:\n",
    "\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "Поиск вести по всей доступной preview-информации (это информация, доступная непосредственно с текущей страницы).\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <дата> - <заголовок> - <ссылка>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-01 09:40:00</td>\n",
       "      <td>\\n    как мы научили робота чувству юмора\\n</td>\n",
       "      <td>https://habr.com/ru/company/funcorp/blog/517182/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date                                          Title  \\\n",
       "0 2020-09-01 09:40:00  \\n    как мы научили робота чувству юмора\\n     \n",
       "\n",
       "                                               Link  \n",
       "0  https://habr.com/ru/company/funcorp/blog/517182/  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locale.setlocale(locale.LC_TIME, 'ru_RU.UTF-8')\n",
    "KEYWORDS = {'python', 'парсинг', 'JavaScript'}\n",
    "\n",
    "df = pd.DataFrame(columns=['Date', 'Title', 'Link'])\n",
    "current_row = 0\n",
    "\n",
    "# get rescent posts\n",
    "res = requests.get('https://habr.com/ru/all/')\n",
    "soup = BeautifulSoup(res.text, 'html5lib')\n",
    "posts = soup.find_all('article', class_='post')\n",
    "\n",
    "for post in posts:\n",
    "    # check if a post have id\n",
    "    post_id = post.parent.attrs.get('id')\n",
    "    if not post_id:\n",
    "        continue\n",
    "        \n",
    "    # find and check all preview information\n",
    "    title = post.find('h2', class_='post__title').text.lower()\n",
    "    kw_in_title = [kw in title for kw in KEYWORDS]\n",
    "    \n",
    "    preview_text = post.find('div', class_='post__text').text.lower()\n",
    "    kw_in_preview = [kw in preview_text for kw in KEYWORDS]\n",
    "    \n",
    "    hubs = post.find_all('a', class_='hub-link')\n",
    "    current_hubs = set()\n",
    "    for hub in hubs:\n",
    "        current_hubs.add(hub.text.lower())\n",
    "        \n",
    "    if current_hubs.intersection(KEYWORDS) or any(kw_in_title) or any(kw_in_preview):\n",
    "        # parse link\n",
    "        post_header = post.find('a', class_ = 'post__title_link')\n",
    "        link = post_header.attrs.get('href')\n",
    "\n",
    "        # obtain date-time\n",
    "        datetime_block = post.find_all('span', class_='post__time')\n",
    "        datetime_string = datetime_block[0].contents[0]\n",
    "        try:\n",
    "            dt = pd.to_datetime(datetime_string, format='%d %B %Y в %H:%M')\n",
    "        except ValueError:\n",
    "            if 'сегодня' in datetime_string:\n",
    "                date = datetime.date.today()\n",
    "            elif 'вчера' in datetime_string:\n",
    "                date = datetime.date.today() - datetime.timedelta(days=1)\n",
    "            time_str = datetime_string.split()[-1]\n",
    "            dt = pd.to_datetime(str(date) + ' ' + time_str)\n",
    "        # update dataframe\n",
    "        df.loc[current_row] = dt, title, link, \n",
    "        current_row += 1\n",
    "locale.setlocale(locale.LC_TIME, 'en_US.UTF-8')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительная часть (необязательная)\n",
    "\n",
    "Улучшить скрипт так, чтобы он анализировал не только preview-информацию статьи, но и весь текст статьи целиком.\n",
    "\n",
    "Для этого потребуется получать страницы статей и искать по тексту внутри этой страницы.\n",
    "\n",
    "Итоговый датафрейм формировать со столбцами: <дата> - <заголовок> - <ссылка> - <текст статьи>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>hubs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-01 10:00:00</td>\n",
       "      <td>Awesome-лист своими руками, или GitHub вместо ...</td>\n",
       "      <td>https://habr.com/ru/company/croc/blog/496594/</td>\n",
       "      <td>{github, администрирование баз данных, блог ко...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-01 09:40:00</td>\n",
       "      <td>Как мы научили робота чувству юмора</td>\n",
       "      <td>https://habr.com/ru/company/funcorp/blog/517182/</td>\n",
       "      <td>{python, машинное обучение, блог компании func...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date                                              Title  \\\n",
       "0 2020-09-01 10:00:00  Awesome-лист своими руками, или GitHub вместо ...   \n",
       "1 2020-09-01 09:40:00                Как мы научили робота чувству юмора   \n",
       "\n",
       "                                               Link  \\\n",
       "0     https://habr.com/ru/company/croc/blog/496594/   \n",
       "1  https://habr.com/ru/company/funcorp/blog/517182/   \n",
       "\n",
       "                                                hubs  \n",
       "0  {github, администрирование баз данных, блог ко...  \n",
       "1  {python, машинное обучение, блог компании func...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locale.setlocale(locale.LC_TIME, 'ru_RU.UTF-8')\n",
    "KEYWORDS = {'python', 'парсинг', 'JavaScript'}\n",
    "\n",
    "df = pd.DataFrame(columns=['Date', 'Title', 'Link', 'hubs'])\n",
    "current_row = 0\n",
    "\n",
    "res = requests.get('https://habr.com/ru/all/')\n",
    "soup = BeautifulSoup(res.text, 'html5lib')\n",
    "posts = soup.find_all('article', class_='post')\n",
    "\n",
    "for post in posts:\n",
    "    post_id = post.parent.attrs.get('id')\n",
    "    if not post_id:\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    hubs = post.find_all('a', class_='hub-link')\n",
    "    current_hubs = set()\n",
    "    for hub in hubs:\n",
    "        current_hubs.add(hub.contents[0].lower())\n",
    "        \n",
    "    post_header = post.find_all('a', class_ = 'post__title_link')\n",
    "    link = post_header[0].attrs.get('href')\n",
    "    title = post_header[0].contents[0]\n",
    "    \n",
    "    current_req = requests.get(link)\n",
    "    current_soup = BeautifulSoup(current_req.text, 'html.parser')\n",
    "    post_text = current_soup.find(\"div\", {\"class\": \"post__text\"}).text # from https://habr.com/ru/post/346198/\n",
    "    kw_in_text = [kw in post_text for kw in KEYWORDS]\n",
    "    kw_in_title = [kw in title for kw in KEYWORDS]\n",
    "    \n",
    "    if current_hubs.intersection(KEYWORDS) or any(kw_in_text) or any(kw_in_title):\n",
    "        post_header = post.find_all('a', class_ = 'post__title_link')\n",
    "        link = post_header[0].attrs.get('href')\n",
    "        title = post_header[0].contents[0]\n",
    "\n",
    "        datetime_block = post.find_all('span', class_='post__time')\n",
    "        datetime_string = datetime_block[0].contents[0]\n",
    "        try:\n",
    "            dt = pd.to_datetime(datetime_string, format='%d %B %Y в %H:%M')\n",
    "        except ValueError:\n",
    "            if 'сегодня' in datetime_string:\n",
    "                date = datetime.date.today()\n",
    "            elif 'вчера' in datetime_string:\n",
    "                date = datetime.date.today() - datetime.timedelta(days=1)\n",
    "            time_str = datetime_string.split()[-1]\n",
    "            dt = pd.to_datetime(str(date) + ' ' + time_str)\n",
    "        df.loc[current_row] = dt, title, link, current_hubs\n",
    "        current_row += 1\n",
    "locale.setlocale(locale.LC_TIME, 'en_US.UTF-8')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.\n",
    "### Обязательная часть\n",
    "\n",
    "Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Check. Список email-ов задаем переменной в начале кода:\n",
    "EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <почта> - <дата утечки> - <источник утечки> - <описание утечки>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAILS = ['xxx@x.ru', 'yyy@y.com']\n",
    "\n",
    "res = requests.post('https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data',\n",
    "                    headers = {'Vaar-Header-App-Product': 'hackcheck-web-avast', 'Vaar-Version': '0'},\n",
    "                    json = {\"emailAddresses\":EMAILS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>breach_date</th>\n",
       "      <th>breach_source</th>\n",
       "      <th>breach_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2019-03-28T00:00:00Z</td>\n",
       "      <td>verifications.io</td>\n",
       "      <td>Big data e-mail verification platform verifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2020-05-21T00:00:00Z</td>\n",
       "      <td>vk.com</td>\n",
       "      <td>At some time in 2020, the Russian social netwo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2017-02-14T00:00:00Z</td>\n",
       "      <td>parapa.mail.ru</td>\n",
       "      <td>In July and August 2016, two criminals execute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2016-10-29T00:00:00Z</td>\n",
       "      <td>vk.com</td>\n",
       "      <td>Popular Russian social networking platform VKo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2016-10-21T00:00:00Z</td>\n",
       "      <td>adobe.com</td>\n",
       "      <td>In October of 2013, criminals penetrated Adobe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2017-02-14T00:00:00Z</td>\n",
       "      <td>cfire.mail.ru</td>\n",
       "      <td>In July and August of 2016, two criminals carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2017-01-31T00:00:00Z</td>\n",
       "      <td>cdprojektred.com</td>\n",
       "      <td>In March 2016, CDProjektRed.com.com's forum da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2016-10-23T00:00:00Z</td>\n",
       "      <td>imesh.com</td>\n",
       "      <td>In June 2016, a cache of over 51 million user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2019-03-28T00:00:00Z</td>\n",
       "      <td>verifications.io</td>\n",
       "      <td>Big data e-mail verification platform verifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2019-02-21T00:00:00Z</td>\n",
       "      <td>www.dangdang.com</td>\n",
       "      <td>This is a list of email addresses only, and as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2020-01-03T00:00:00Z</td>\n",
       "      <td>azcentral.com</td>\n",
       "      <td>At an unconfirmed date, online Arizona newspap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2020-05-28T00:00:00Z</td>\n",
       "      <td>wishbone.io</td>\n",
       "      <td>In January 2020, the online poll website Wishb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2017-11-04T00:00:00Z</td>\n",
       "      <td>myheritage.com</td>\n",
       "      <td>In October 2017, a customer database belonging...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2017-12-01T00:00:00Z</td>\n",
       "      <td>creocommunity.com</td>\n",
       "      <td>At an unconfirmed date, Creo Community's user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2019-06-13T00:00:00Z</td>\n",
       "      <td>canva.com</td>\n",
       "      <td>In May 2019, graphic-design site Canva's datab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2016-10-24T00:00:00Z</td>\n",
       "      <td>dropbox.com</td>\n",
       "      <td>Cloud storage company Dropbox suffered a major...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2016-10-21T00:00:00Z</td>\n",
       "      <td>linkedin.com</td>\n",
       "      <td>In 2012, online professional networking platfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2017-03-01T00:00:00Z</td>\n",
       "      <td>rayli.com.cn</td>\n",
       "      <td>On an unconfirmed date, Chinese gossip site Ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2019-10-17T00:00:00Z</td>\n",
       "      <td>zynga.com</td>\n",
       "      <td>In September 2019, the game developer Zynga wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2019-07-11T00:00:00Z</td>\n",
       "      <td>medicaresupplement.com</td>\n",
       "      <td>In May 2019, a security researcher discovered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2016-10-21T00:00:00Z</td>\n",
       "      <td>adobe.com</td>\n",
       "      <td>In October of 2013, criminals penetrated Adobe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2018-02-18T00:00:00Z</td>\n",
       "      <td>netlog.com</td>\n",
       "      <td>Netlog (formerly known as Facebox and Bingbox)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2017-03-15T00:00:00Z</td>\n",
       "      <td>globalreach.eu</td>\n",
       "      <td>In 2016, Global Reach Technology's database wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2016-10-23T00:00:00Z</td>\n",
       "      <td>imesh.com</td>\n",
       "      <td>In June 2016, a cache of over 51 million user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2017-03-24T00:00:00Z</td>\n",
       "      <td>youku.com</td>\n",
       "      <td>Youku is a large Chinese video content company...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        email           breach_date           breach_source  \\\n",
       "0    xxx@x.ru  2019-03-28T00:00:00Z        verifications.io   \n",
       "1    xxx@x.ru  2020-05-21T00:00:00Z                  vk.com   \n",
       "2    xxx@x.ru  2017-02-14T00:00:00Z          parapa.mail.ru   \n",
       "3    xxx@x.ru  2016-10-29T00:00:00Z                  vk.com   \n",
       "4    xxx@x.ru  2016-10-21T00:00:00Z               adobe.com   \n",
       "5    xxx@x.ru  2017-02-14T00:00:00Z           cfire.mail.ru   \n",
       "6    xxx@x.ru  2017-01-31T00:00:00Z        cdprojektred.com   \n",
       "7    xxx@x.ru  2016-10-23T00:00:00Z               imesh.com   \n",
       "8   yyy@y.com  2019-03-28T00:00:00Z        verifications.io   \n",
       "9   yyy@y.com  2019-02-21T00:00:00Z        www.dangdang.com   \n",
       "10  yyy@y.com  2020-01-03T00:00:00Z           azcentral.com   \n",
       "11  yyy@y.com  2020-05-28T00:00:00Z             wishbone.io   \n",
       "12  yyy@y.com  2017-11-04T00:00:00Z          myheritage.com   \n",
       "13  yyy@y.com  2017-12-01T00:00:00Z       creocommunity.com   \n",
       "14  yyy@y.com  2019-06-13T00:00:00Z               canva.com   \n",
       "15  yyy@y.com  2016-10-24T00:00:00Z             dropbox.com   \n",
       "16  yyy@y.com  2016-10-21T00:00:00Z            linkedin.com   \n",
       "17  yyy@y.com  2017-03-01T00:00:00Z            rayli.com.cn   \n",
       "18  yyy@y.com  2019-10-17T00:00:00Z               zynga.com   \n",
       "19  yyy@y.com  2019-07-11T00:00:00Z  medicaresupplement.com   \n",
       "20  yyy@y.com  2016-10-21T00:00:00Z               adobe.com   \n",
       "21  yyy@y.com  2018-02-18T00:00:00Z              netlog.com   \n",
       "22  yyy@y.com  2017-03-15T00:00:00Z          globalreach.eu   \n",
       "23  yyy@y.com  2016-10-23T00:00:00Z               imesh.com   \n",
       "24  yyy@y.com  2017-03-24T00:00:00Z               youku.com   \n",
       "\n",
       "                                   breach_description  \n",
       "0   Big data e-mail verification platform verifica...  \n",
       "1   At some time in 2020, the Russian social netwo...  \n",
       "2   In July and August 2016, two criminals execute...  \n",
       "3   Popular Russian social networking platform VKo...  \n",
       "4   In October of 2013, criminals penetrated Adobe...  \n",
       "5   In July and August of 2016, two criminals carr...  \n",
       "6   In March 2016, CDProjektRed.com.com's forum da...  \n",
       "7   In June 2016, a cache of over 51 million user ...  \n",
       "8   Big data e-mail verification platform verifica...  \n",
       "9   This is a list of email addresses only, and as...  \n",
       "10  At an unconfirmed date, online Arizona newspap...  \n",
       "11  In January 2020, the online poll website Wishb...  \n",
       "12  In October 2017, a customer database belonging...  \n",
       "13  At an unconfirmed date, Creo Community's user ...  \n",
       "14  In May 2019, graphic-design site Canva's datab...  \n",
       "15  Cloud storage company Dropbox suffered a major...  \n",
       "16  In 2012, online professional networking platfo...  \n",
       "17  On an unconfirmed date, Chinese gossip site Ra...  \n",
       "18  In September 2019, the game developer Zynga wa...  \n",
       "19  In May 2019, a security researcher discovered ...  \n",
       "20  In October of 2013, criminals penetrated Adobe...  \n",
       "21  Netlog (formerly known as Facebox and Bingbox)...  \n",
       "22  In 2016, Global Reach Technology's database wa...  \n",
       "23  In June 2016, a cache of over 51 million user ...  \n",
       "24  Youku is a large Chinese video content company...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "info = json.loads(res.text)\n",
    "\n",
    "df = pd.DataFrame(columns=['email', 'breach_date', 'breach_source', 'breach_description'])\n",
    "current_row = 0\n",
    "\n",
    "for email in EMAILS:\n",
    "    for breach_id in info['summary'][email]['breaches']:\n",
    "        breach = info['breaches'][str(breach_id)]\n",
    "        df.loc[current_row] = email, breach['publishDate'], breach['site'], breach['description']\n",
    "        current_row += 1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительная часть (необязательная)\n",
    "\n",
    "Написать скрипт, который будет получать 50 последних постов указанной группы во Вконтакте.\n",
    "Документация к API VK: https://vk.com/dev/methods , вам поможет метод wall.getGROUP = 'netology'\n",
    "TOKEN = УДАЛЯЙТЕ В ВЕРСИИ ДЛЯ ПРОВЕРКИ, НА GITHUB НЕ ВЫКЛАДЫВАТЬ\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <дата поста> - <текст поста>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWSFEED_REQUEST = 'https://api.vk.com/method/wall.get?'\n",
    "\n",
    "VERSION = '5.103'\n",
    "SLEEP = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'access_token': TOKEN,\n",
    "    'v': VERSION,\n",
    "    'group': 'netology',\n",
    "    'owner_id':'-59793580',\n",
    "    'count': 50\n",
    "}\n",
    "res = requests.get(NEWSFEED_REQUEST, params)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-01 07:10:00</td>\n",
       "      <td>Гренландия 16+ \\n С 3 сентября Кинотеатр Комсо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-31 15:10:00</td>\n",
       "      <td>Вратарь Галактики 6+ \\nС 3 сентября Кинотеатр ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-31 10:10:00</td>\n",
       "      <td>Довод 18+\\nС 3 сентября Кинотеатр Комсомолец\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-31 09:12:29</td>\n",
       "      <td>Поезд в Пусан 2: Полуостров 18+\\nС 3 сентября ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-28 07:13:51</td>\n",
       "      <td>✅Дорогие друзья, присоединяйтесь к голосованию!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-08-27 11:17:00</td>\n",
       "      <td>Тролли. Мировой тур 2D 6+ \\nСовсем скоро!!! Ки...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-08-27 11:03:56</td>\n",
       "      <td>27 АВГУСТА ДЕНЬ РОССИЙСКОГО КИНО!!! 😁\\nВСЕХ С ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-08-27 09:08:09</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-08-07 12:35:02</td>\n",
       "      <td>В начале месяца, 1 августа 2020, как сообщалос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-07-01 14:58:39</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-07-01 06:11:32</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-06-20 03:23:23</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-06-16 16:52:33</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-06-12 07:31:01</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-06-05 10:31:27</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-06-03 06:11:42</td>\n",
       "      <td>Ждем, надеемся и верим! В зависимости от стати...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-05-19 06:10:55</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-05-09 13:57:06</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-05-09 13:34:29</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-05-09 13:04:48</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020-05-09 12:48:27</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2020-05-09 12:31:43</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2020-05-09 12:19:19</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020-05-09 12:05:16</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-05-09 11:55:52</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2020-05-09 11:32:01</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2020-05-09 11:25:35</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020-05-09 11:04:04</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020-05-09 10:55:17</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2020-05-09 10:32:11</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2020-05-09 10:20:59</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2020-05-09 10:00:33</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2020-05-09 09:46:41</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2020-05-09 09:32:23</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020-05-09 09:18:07</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2020-05-09 09:09:26</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2020-05-09 08:53:58</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2020-05-09 08:33:04</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2020-05-09 08:24:01</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2020-05-09 08:23:53</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020-05-09 08:23:46</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2020-05-09 08:23:32</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2020-05-09 07:53:54</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2020-05-09 07:11:46</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2020-05-08 18:45:05</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2020-05-08 15:02:16</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2020-05-04 05:06:49</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2020-05-02 16:12:18</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2020-04-30 03:42:13</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2020-04-24 12:59:15</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                               text\n",
       "0  2020-09-01 07:10:00  Гренландия 16+ \\n С 3 сентября Кинотеатр Комсо...\n",
       "1  2020-08-31 15:10:00  Вратарь Галактики 6+ \\nС 3 сентября Кинотеатр ...\n",
       "2  2020-08-31 10:10:00  Довод 18+\\nС 3 сентября Кинотеатр Комсомолец\\n...\n",
       "3  2020-08-31 09:12:29  Поезд в Пусан 2: Полуостров 18+\\nС 3 сентября ...\n",
       "4  2020-08-28 07:13:51  ✅Дорогие друзья, присоединяйтесь к голосованию!!!\n",
       "5  2020-08-27 11:17:00  Тролли. Мировой тур 2D 6+ \\nСовсем скоро!!! Ки...\n",
       "6  2020-08-27 11:03:56  27 АВГУСТА ДЕНЬ РОССИЙСКОГО КИНО!!! 😁\\nВСЕХ С ...\n",
       "7  2020-08-27 09:08:09                                                   \n",
       "8  2020-08-07 12:35:02  В начале месяца, 1 августа 2020, как сообщалос...\n",
       "9  2020-07-01 14:58:39                                                   \n",
       "10 2020-07-01 06:11:32                                                   \n",
       "11 2020-06-20 03:23:23                                                   \n",
       "12 2020-06-16 16:52:33                                                   \n",
       "13 2020-06-12 07:31:01                                                   \n",
       "14 2020-06-05 10:31:27                                                   \n",
       "15 2020-06-03 06:11:42  Ждем, надеемся и верим! В зависимости от стати...\n",
       "16 2020-05-19 06:10:55                                                   \n",
       "17 2020-05-09 13:57:06                                                   \n",
       "18 2020-05-09 13:34:29                                                   \n",
       "19 2020-05-09 13:04:48                                                   \n",
       "20 2020-05-09 12:48:27                                                   \n",
       "21 2020-05-09 12:31:43                                                   \n",
       "22 2020-05-09 12:19:19                                                   \n",
       "23 2020-05-09 12:05:16                                                   \n",
       "24 2020-05-09 11:55:52                                                   \n",
       "25 2020-05-09 11:32:01                                                   \n",
       "26 2020-05-09 11:25:35                                                   \n",
       "27 2020-05-09 11:04:04                                                   \n",
       "28 2020-05-09 10:55:17                                                   \n",
       "29 2020-05-09 10:32:11                                                   \n",
       "30 2020-05-09 10:20:59                                                   \n",
       "31 2020-05-09 10:00:33                                                   \n",
       "32 2020-05-09 09:46:41                                                   \n",
       "33 2020-05-09 09:32:23                                                   \n",
       "34 2020-05-09 09:18:07                                                   \n",
       "35 2020-05-09 09:09:26                                                   \n",
       "36 2020-05-09 08:53:58                                                   \n",
       "37 2020-05-09 08:33:04                                                   \n",
       "38 2020-05-09 08:24:01                                                   \n",
       "39 2020-05-09 08:23:53                                                   \n",
       "40 2020-05-09 08:23:46                                                   \n",
       "41 2020-05-09 08:23:32                                                   \n",
       "42 2020-05-09 07:53:54                                                   \n",
       "43 2020-05-09 07:11:46                                                   \n",
       "44 2020-05-08 18:45:05                                                   \n",
       "45 2020-05-08 15:02:16                                                   \n",
       "46 2020-05-04 05:06:49                                                   \n",
       "47 2020-05-02 16:12:18                                                   \n",
       "48 2020-04-30 03:42:13                                                   \n",
       "49 2020-04-24 12:59:15                                                   "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [[pd.to_datetime(x['date'], unit='s'), x['text']] for x in res.json()['response']['items']],\n",
    "    columns = ['date', 'text']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
